import copy
import random
import constraints
from math import log, e
import numpy as np

# constraints_dict is dict where keys are constraint function names
# and values are (boolean, int) tuples representing (enabled, cost)

def evaluation_func(assignment, domain, constraints_dict):
    total_cost = 0
    for key in constraints_dict.keys():
        enabled, cost = constraints_dict[key]
        if enabled:
            total_cost += getattr(constraints, key)(assignment, domain, cost)
    return total_cost

# the assignment is represented as a dict where the key is a worker
# and the value is a list of the tasks assigned to that worker

# initialize a new assignment by making sure every task has at least one worker
# more specifically, a random number of workers between 1 and len(workers)

def init_assignment(domain):
    workers, tasks = domain
    assignment = {}
    flipped_asst = {}
    for task in tasks.values():
        n = task.num_workers
        if n > len(workers):
            assignment[task.name] = workers
        else:
            assignment[task.name] = random.sample(workers, n)
    for worker in workers.keys():
        flipped_asst[worker] = []
    for task in assignment.keys():
        for worker in assignment[task]:
            flipped_asst[worker].append(task)
    return flipped_asst

# neighbors of the current assignment are generated in one of two ways:
# either remove one of a worker's assigned tasks or assign the worker a new task

def find_all_neighbors(assignment, domain, constraints_dict):
    workers, tasks = domain
    neighbors = []
    for worker in assignment.keys():
        print worker
        for task in tasks.values():
            new_assignment = copy.deepcopy(assignment)
            if task.name in assignment[worker]:
                new_assignment[worker].remove(task.name)
                cost = evaluation_func(new_assignment, domain, constraints_dict)
            else:
                new_assignment[worker].append(task.name)
                cost = evaluation_func(new_assignment, domain, constraints_dict)
            neighbors.append((new_assignment, cost))
    return neighbors

# we evaluate all neighbors of the current assignment
# and then we return the "best" neighbor (with the lowest cost)

def find_best_neighbor(assignment, domain, constraints_dict, best_cost):
    workers, tasks = domain
    best_assignment = copy.deepcopy(assignment)
    for worker in assignment.keys():
        for task in tasks.values():
            new_assignment = copy.deepcopy(assignment)
            if task.name in assignment[worker]:
                new_assignment[worker].remove(task.name)
                cost = evaluation_func(new_assignment, domain, constraints_dict)
            else:
                new_assignment[worker].append(task.name)
                cost = evaluation_func(new_assignment, domain, constraints_dict)
            #print "Cost:", cost
            #print "Best Cost:", best_cost
            if cost < best_cost:
                best_cost = cost
                best_assignment = new_assignment
                #print "New assignment with cost", best_cost
                #print best_assignment
    return (best_assignment, best_cost)
    
# function to choose a random neighbor from the set
# of all possible neighbors of the current assignment

def find_random_neighbor(assignment, domain, constraints_dict):
    workers, tasks = domain
    new_assignment = copy.deepcopy(assignment)
    worker = random.choice(workers.values())
    task = random.choice(tasks.values())
    if task.name in assignment[worker.name]:
        new_assignment[worker.name].remove(task.name)
    else:
        new_assignment[worker.name].append(task.name)
    cost = evaluation_func(new_assignment, domain, constraints_dict)
    return (new_assignment, cost)

# for hill_climbing, we start with a random assignment generated by init_assignment
# then we repeatedly move from the current assignment to the best neighbor
# until we find a local maxima, which we then return as a solution

# hill climbing
def hill_climbing(domain, constraints_dict):
    assignment = init_assignment(domain)
    best_cost = evaluation_func(assignment, domain, constraints_dict)
    best_neighbor, neighbor_cost = find_best_neighbor(assignment, domain, constraints_dict, best_cost)
    while(neighbor_cost < best_cost):
        assignment = best_neighbor
        best_cost = neighbor_cost
        best_neighbor, neighbor_cost = find_best_neighbor(assignment, domain, constraints_dict, best_cost)
        print "cost", best_cost
    print "FINAL COST", neighbor_cost
    return best_neighbor

# random restart hill climbing
def rr_hill_climbing(domain, constraints_dict):
    # SET MAX ITERATIONS HERE
    max_iters = 50
    best_result = (None, float("inf"))
    for i in xrange(max_iters):
        assignment = init_assignment(domain)
        best_cost = float("inf")
        best_neighbor, neighbor_cost = find_best_neighbor(assignment, domain, constraints_dict, best_cost)
        while(neighbor_cost < best_cost):
            assignment = best_neighbor
            best_cost = neighbor_cost
            best_neighbor, neighbor_cost = find_best_neighbor(assignment, domain, constraints_dict, best_cost)
        if neighbor_cost < best_result[1]:
            best_result = (best_neighbor, neighbor_cost)
    print "FINAL COST", best_result[1]
    return best_result[0]

# GLOBALS
TEMP_FUNCTION = 0 # 0 for exp, 1 for fast, 2 for boltz
INIT_TEMP = 1000
MIN_TEMP = 1

# stochastic hill climbing
def stoc_hill_climbing(domain, constraints_dict):
    # SET MAX ITERATIONS HERE
    max_iters = 50
    assignment = init_assignment(domain)
    curr_cost = evaluation_func(assignment, domain, constraints_dict)
    for i in xrange(max_iters):
        neighbors = find_all_neighbors(assignment, domain, constraints_dict)
        diffs = map(lambda x: curr_cost - x[1], neighbors)
        diffs = map(lambda x: 0 if x < 0 else x, diffs)
        diff_sum = float(sum(diffs))
        if diff_sum == 0.0: # local maximum
            print "FINAL COST", curr_cost
            return assignment
        else:
            probs = map(lambda x: x / diff_sum, diffs)
            idx = list(np.random.multinomial(1, probs, 1)[0]).index(1)
            assignment = neighbors[idx][0]
            curr_cost = neighbors[idx][1]
        print "cost", curr_cost
    print "FINAL COST", curr_cost
    return assignment

def temperature(k):
    if TEMP_FUNCTION == 0: # exponential
        return INIT_TEMP * 0.9995**k
    elif TEMP_FUNCTION == 1: # fast
        return INIT_TEMP / k
    elif TEMP_FUNCTION == 2: # boltz
        return INIT_TEMP / log(k)
    else:
        raise("Invalid TEMP_FUNCTION")

def p_move(new_cost, cost, temp):
    return e**((new_cost-cost)/temp)

# simulated annealing
def simulated_annealing(domain, constraints_dict):
    assignment = init_assignment(domain)
    cost = evaluation_func(assignment, domain, constraints_dict)
    best_assignment, lowest_cost = assignment, cost
    k = 0
    while True:
        print k
        print "cost", cost
        print "lowest_cost", lowest_cost
        temp = temperature(k)
        if temp < MIN_TEMP:
            return best_assignment
        rand_neighbor, new_cost = find_random_neighbor(assignment, domain, constraints_dict)
        if new_cost < cost or p_move(new_cost, cost, temp) < random.random():
            assignment = rand_neighbor
            cost = new_cost
            if cost < lowest_cost:
                best_assignment = assignment
                lowest_cost = cost
        k += 1

# random stochastic hill climbing
def rand_stoc_hill_climbing(domain, constraints_dict):
    assignment = init_assignment(domain)
    cost = evaluation_func(assignment, domain, constraints_dict)
    best_assignment, lowest_cost = assignment, cost
    # SET MAX ITERATIONS HERE
    max_iters = 50000
    for i in xrange(max_iters):
        print i
        print "cost", cost
        print "lowest_cost", lowest_cost
        rand_neighbor, new_cost = find_random_neighbor(assignment, domain, constraints_dict)
        if new_cost < cost and e**(new_cost-cost) < random.random():
            assignment = rand_neighbor
            cost = new_cost
            if cost < lowest_cost:
                best_assignment = assignment 
                lowest_cost = cost
    return best_assignment

'''
A cross between Beam Search and Local Search. Normally used to maximize an objective function.
The algorithm holds 'k' number of states at any given time. Initially these k states are
randomly generated. The successors of these k states are calculated using the objective function.
If any of these successors is a 'goal', that is, the maximum value of the objective function,
then the algorithm halts. Otherwise the initial k states and k number of successors are placed in a pool.
This pool has a total of 2k states. The pool is numerically sorted and the best (highest) k states
are selected as new initial states. This process repeats until a maximum value is reached.
This algorithm is particularity effective at quickly abandoning 'dead end' searches, so maximum
resources can be used on the promising successors. However when using the Local Beam Searching
algorithm, the k states can easily become concentrated over a very small amount of state space.
This leads to the algorithm being nothing more than a more resource intensive Hill Climbing algorithm.
'''
def beam_search(domain, constraints_dict, k=10):
    initial_assignments = [init_assignment(domain) for i in range(k)]
    min_k = initial_assignments
    successors = []
    scores = []
    max_iters = 30000
    for i in xrange(max_iters):
        for asst in min_k:
            neighbors = find_all_neighbors(asst, domain, constraints_dict)
            print neighbors
            successors.append(neighbors)
        successors = [item for sublist in successors for item in sublist]
        for asst in (successors + initial_assignments):
            score = evaluation_func(asst, domain, constraints_dict)
            if score == 0:
                return asst
            scores.append(score, asst)
        min_k = scores.sort()[k:]
        print min_k
    
        
        
    
    
>>>>>>> 58f205ff975329f5bb6415a4db6cce9e1ae78842
